%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%                                                                 %
%%%%%     <file_name>.tex                                             %
%%%%%                                                                 %
%%%%% Author:      <author>                                           %
%%%%% Created:     <date>                                             %
%%%%% Description: <description>                                      %
%%%%%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Results}
\section{Test Setup}

    To benchmark Halide, I used a simple matrix multiplication program.
    We ported the pipeline code from the example in the Halide repository and created the \gls{hero} application around it.
    We chose to benchmark a matrix multiplication as it is a common workflow in signal processing.

\lstset{basicstyle=\ttfamily\footnotesize,breaklines=true,tabsize=2}
\begin{lstlisting}[caption={Matrix Multiplication Pipeline.}, captionpos=b, label={code:matmul_pipeline}]
    ImageParam A(type_of<int>(), 2);
    ImageParam B(type_of<int>(), 2);
    Var x, y;
    Func matrix_mul("matrix_mul");
    Func out;

    RDom k( 0,A.width() );

    matrix_mul(x, y) += A(x, k) * B(k, y);

    out(x, y) = matrix_mul(x, y);



\end{lstlisting}

    The Listing~\ref{code:matmul_pipeline} shows the full algorithm implementation. We first defined two images parameters: \texttt{A} and \texttt{B}. and initialize the two main variables \texttt{x} and \texttt{y}.
    Then we create a reduction domain, this object defines a domain over which to iterate.
    This way we can easily bound the sum for each coefficient. 
    Then we compute each coefficient with the function \texttt{matrix\_mul(x,y)}.
    The mathematical expression for each coefficient is : $c_{i,j} = \sum^{k=n}_{k=0} a_{ik}*b_{kj}$.
    We can express is quite similarly using Halide, the program will iterate over the variable \texttt{k} repeating the same operation (here a multiplication and accumulation).
    Then we put the final value inside the output buffer.
    
    To have an idea of the performance, we ran a similar application using \gls{openmp}, and compared the total execution time of both applications.
    To ensure that the results were correct, the output was precomputed and we compared it to the output matrix.
    For both applications, the matrices were randomly generated and stored inside the L1 cache.
    The execution time was measured using \texttt{hero\_get\_clk\_counter()} and \texttt{hero\_reset\_clk\_counter()}.
    These two functions respectively reset a cycle counter and return the counter's value when called. 
    Both applications were benchmarked on the hardware simulator.
\section{Halide Results}

% Table of the values
\begin{figure}[H]
        \center
        \scalebox{.8}{\input{figures_raw/Barplot_Halide.pgf}}
    \caption{Halide results relative to Halide base schedule performance.}
    \label{fig:barPlotHalide}
\end{figure}



\begin{figure}[H]
        \center
    \scalebox{.8}{\input{figures_raw/Roofline_Halide.pgf}}
    \caption{Roofline plot for Halide.}
    \label{fig:rooflineHalide}
\end{figure}

\input{content/BenchmarksHalide.tex}

    The figures~\ref{fig:barPlotHalide} and \ref{fig:rooflineHalide} show the results of Halide for different matrix sizes ranging from 15 to 40.
    The benchmarks used three different schedules: the default one, with no parallelization, the parallel schedule along the y-axis, and one schedule which combines a parallel schedule with a vectorize one, the third one is a more optimized schedule, we will talk about it in greater detail in section~\nameref{sec:optimization}.

    The bar plot~\ref{fig:barPlotHalide}, compares the gain in performance for each schedule relative to the base Halide schedule. When parallelizing the multiplication, we observe between five and eight times better performances. When the matrix size is a multiple of eight (the number of cores), we get the best scaling possible.
    This is due to the way Halide handle task distribution, if the number of tasks is not a multiple of the number of cores, some core will stay inactive waiting for the other to complete their tasks.
    For example, for a matrix of size twenty-five, during three rounds, every core will compute three coefficients, and the first core will compute four coefficients.
    So during the last round, only one core will be working, reducing the scaling of the program.
    If we compute the average number of coefficient computed per round, for a matrix of size twenty-five, we get: $\frac{25}{4}=6.25$.
     This average matches the increase of performance we observed in our benchmarks.
     The parallel schedule works best when the number of tasks can be divided by the number of available cores.


    Figure~\ref{fig:rooflineHalide} shows the performance of Halide compared to an ideal scenario. In this case, an ideal core can achieve one arithmetic operation per cycle (which can be done by a \gls{pulp} cluster as the multiplications can be pipelined).
So the total number of operations needed to complete one matrix multiplication is: $2n^3$.
    The compute intensity of the program represents the number of operations we execute on each byte. We execute $2n^3$ operations, and  $3n^2$ memory load / write of 32-bit integers, the overall computational intensity of the program is: $\frac{2n^3}{12n^2}=\frac{n}{6}$.

    The default schedule performs identically independently of the matrix size, but the performance of the parallel schedule and the vectorize one increase with the size.
    This is due to the overlap in memory access, as every core can read four bytes in the  L1 cache every cycle, by overlapping the requests, we decrease the time loss due to the memory latency.


%%%%%% CHANGE |
%%%%%%        v
    
\section{Comparison with an already working toolchain: \acrshort{openmp}.}

\begin{figure}[H]
        \center
    \scalebox{.6}{\input{figures_raw/RooflineHalideOpenMp.pgf}}
    \caption{Roofline plot for Halide}
    \label{fig:rooflineHalideOpenMP}
\end{figure}
\begin{figure}[H]
        \center
        \scalebox{.6}{\input{figures_raw/BarplotHalideOpenMp.pgf}}
    \caption{Halide results relative to Halide base schedule performance.}
    \label{fig:barPlotHalideOpenMP}
\end{figure}

\input{content/BenchmarksHalideOpenMP.tex}
    
    Currently, the cluster is handled by \gls{openmp}, we decided to compare both solutions to see how Halide performed against it.
    Table~\ref{Table:BenchmarksHalideOpenMP} presents the results of both schedules.
    Both applications perform the same using only a  single core.
    The execution time difference is less than four thousand cycles on forty by forty matrices ($0.5 \%$ of the total execution time).

    On the multithreaded run, Halide managed to outperform \gls{openmp}, achieving 40\% more operations per cycles on thirty by thirty matrices and 35 \% more performance on forty by forty matrices.
    It seems that \gls{openmp} catch up to Halide on bigger matrices, but we did not test any further as the goal of the project was not to determine which \gls{api} was the best but to see if we could achieve similar performance to  \gls{openmp} with Halide.


\subsection{Optimizing the schedule of the matrix multiplication}
\label{sec:optimization}
    We first compared the performance of every basic schedule on twenty by twenty matrices, unsurprisingly,  the parallel schedule performed the best, and the vectorized one was close behind.
    Every other schedules (unroll, tile, fused, split) performed slightly worse than the base schedule because they all added additional computations or additional jumps.
    The vectorize schedule performed twice as good as the default schedule even though \gls{llvm} do not support the \gls{pulp} \gls{simd} extension.
    This jump in performance is due to data reutilisation.
    On the split, unroll, and tile schedule, if we look at the generated assembly, the code does not reuse any already loaded coefficient, every value loaded from the cache is used only once.
    Moreover, splitting the code introduce even more loops, which may take more time to execute due to the jump instructions.

    On the other hand, the \texttt{vectorize} schedule loads one coefficient of the first matrix, and a $k$ coefficients of the second one (where $k$ is the vector length specified when programming the schedule) to compute at the same time $k$ coefficients of the output.
    To compute $k$ coefficient, we will do $n(k+1)$ memory load, with $k$ memory store.
    We can compute all the coefficients in: $\frac{n^2}{k}$ iterations, and $n^3\frac{k+1}{k} + n^2 \approx n^3 + n^2$ memory load (instead of: $n^2.(2n+1)=2n^3 + n^2$ memory operations).
    But we can't increase $k$ as much as we want as we only have a limited amount of registers available.
    To see which vector length was the best, We exhaustively tried every vector length possible on twenty by twenty matrices. The schedule used to obtain the results shown by Figure~\ref{Fig:Vectorization}, are achieved using the schedule described in Listing~\ref{code:goodSchedule}

\lstset{basicstyle=\ttfamily\footnotesize,breaklines=true,tabsize=2}

\begin{lstlisting}[caption={Schedule using Parallel and Vectorize}, captionpos=b, label={code:goodSchedule}]
    out.parallel(y);
    out.vectorize(x, k);

\end{lstlisting}

\begin{figure}[H]
    \begin{center}
    \resizebox{10cm}{!}{\input{figures_raw/Vectorize.pgf}}
    \label{Fig:Vectorization}
        \caption{Impact of the vectorization factor on the performance of the application.}
        \label{fig:Vectorization}
    \end{center}
\end{figure}

    We can see on Figure~\ref{fig:Vectorization}, that the execution time starts around 9000 cycles for a vector length of two, and decrease to 7300 cycles for a vector length of eight.
    When the vector length becomes too large, the execution time increases again, because we only have a few registers to work with, and Halide is forced to store some values into the cache when the length is too large.


    If we compare our optimized schedule with the two other schedules tested, we get a significant jump in performance.
    In Figure~\ref{fig:BarPlotHalide} we can see a difference in performance ranging form ten times to twenty times the single thread performance of Halide.
    And compared to the multithreaded application, the gains are still significant, as we achieve between $0.5$ more operations per cycle up to $2.125$ more operations per cycle.


    This may not be the most optimal schedule, and for even bigger matrices, more optimal solutions may be found, but the process to get to this schedule was rudimentary, and without having to change the algorithm, it is really easy to experiment with different schedules and come up with one that performs better.
