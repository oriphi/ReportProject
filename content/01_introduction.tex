%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%                                                                 %
%%%%%     <file_name>.tex                                             %
%%%%%                                                                 %
%%%%% Author:      <author>                                           %
%%%%% Created:     <date>                                             %
%%%%% Description: <description>                                      %
%%%%%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}

	Thanks to the smaller nodes of modern lithography techonlogies and the transistor density we can achieve with them, modern \glspl{cpu} in embedded systems, as capable enough to be used in data center. A single Raspberry Pi 3 has a peak performance of 6 \glspl{gflop} for a power consumption of only 7 Watts~\ref{Art:RPiClusters}.
	Embedded systems can take advantage to becomes more autonomous and not rely on external computer for heavy computation. Nano drones can now analyse in real time a video signal and train a neural network for autonomous navigation under \si{100 mW}~\ref{Art:NanoDrones}.

    To improve the energy efficiency and the computing power of \gls{ulp} systems, new architectures are needed, to keep the power consumption low, an embedded system needs to power it's subsystems only when needed. Autonomous drones~\ref{Art:NanoDrones}, uses a low performance micro controller to manage the drone coupled with a high performance cluster foro the signal processing tasks. System using one host processor and one or multiple coprocessors are called heterogeneous systems. They are suited for embedded systems as they can keep a low power consumption while using high performance accelerator when needed.

This strategy has been used in the \gls{soc} industry by ARM since 2011, The  big.LITTLE architecture~\cite{Art:bigLITTLE} is based on on two clusters of ARM Cortex A7 and A15, and  was designed to increase the computing power in low power systems such as smartphones while increasing the battery life of the device. This architecture relied on a single~ \gls{isa}(ARMv7), the goal was to use the more powerful cores during heavy computation or graphic rendering, and let the low power cores handle the background tasks or manage the device during sleep. Researchers also tried to leverage the advantages of multiple \glspl{isa}, according to this article~\ref{Art:Harnessing}, under heavy design constraints (such as die area or thermal dissipation) heterogeneous systems based on multiple \glspl{isa} performed better than the best homogeneous counterpart (in terms of energy efficiency and compute performance). 
Even in data centers where power comsumption is also an issue, \glspl{gpu} are used thanks to their massive core count and the various \glspl{api} such as \gls{cuda} or \gls{opencl} which make them capable of heavy computation.

    Hero~\cite{Art:Hero} is a heterogeneous system developed by the \gls{iis} of \acrshort{eth} and the \gls{eees} of the University of Bologna. This platform is composed of a hard multicore ARM 64 Juno \gls{soc} (composed of two Cortex A57 and four Cortex A53) and up to eight \gls{pulp} clusters (composed of eight RI5CY cores), running on an \gls{fpga}. 
	The \gls{pulp} cluster is based on the \gls{riscv} \gls{isa}, this \gls{isa} is open source and was designed to support a wide range of platform from embedded systems to supercomputer. The modularity of the \gls{isa} makes it interesting for \glspl{pcma}.

	This platform is designed to ``facilitate rapid exploration on all software and hardware layers '', and includes an heterogeneous compilation toolchain with support for OpenMp.



\section {Design Issue with heterogeneous systems}
	Due to their complexity,heterogeneous systems are difficult to design and program~\ref{Art:Hero}.

    During their conception, numerous design choices need to be made to make sure that all the \gls{cpu} in the system will interact will each other, these choices will impact the peak performance of the design or it's power consumption~\cite{Art:Harnessing}. The computer architect has to choose how  the different \glspl{pcma} will interact, how they will share data, maybe extend the existing \glspl{isa} to distribute tasks, and so on.

    The software design is challenging, when compiling for heterogeneous platforms,  the compiler needs to create an executable that will run on the host processor, but also dedicate parts of the final binary to embed the code that will be distributed on the \glspl{pcma}. Code distribution is handled by the programmer, \glspl{api} such as \gls{cuda}, \gls{opencl} or \gls{openmp}, specific function calls tell the compiler which code to distribute to which \gls{pcma}.


\section {Currently Available Workflow for Halide}
    Currently Hero supports OpenMP, which is an \gls{api} which ``defines a portable, scalable model with a simple and flexible interface for developing parallel applications on platforms from the desktop to the supercomputer''~\cite{Web:OpenMp}. This \gls{api} has been implemented on \gls{hero} to easily take advantage of the \gls{pulp} clusters. The toolchain uses Clang, and  \verb|clang-offload-bundler| to compile the applications. Clang uses a custom front head for \gls{hero} which supports all the available configurations (only the \gls{pulp} cluster for simulation, with the ARM host \gls{cpu}  with a 64 bits \gls{riscv} \gls{cpu}).

	To distribute the code, \gls{openmp} uses preprocessor instructions to tell Clang where the code will run and how it will be executed. Exploring the design space using OpenMp's directive can be time consuming. For example, the developer must explicitely which part of the code to offload by specifing the size of the domain. Trying to change the oder of multiple loops may cause bugs in the algorithm and so on. This approach require to spend time retesting the algorithm before trying other schedules.


	Halide~\cite{Art:Halide} was proposed to explore the idea of separating the algorithm from the schedule.
	This separation make testing new schedule easier on the developer as the algorithm code will stay the same, and only the scheduling will be changed when testing.
    Every processing pipeline designed with Halide has two parts. The first part consist of the functional description of the processing kernel, this is the algorithm that will be executed on the array. The second part is the schedule of the pipeline. This schedule describes how the algorithm will be executed on the system. 
	This programming model is interesting because the developer can implement the algorithm without having to take into account the boundaries of the functions or the border effects. 

	The intermediate variables can be bounded afterward if needed, and the principle variables such as the arrays width and height are automatically bounded by Halide.
	The scheduling process can even be done automatically during the compilation by the library, in order to find an optimal schedule on the target platform.



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../report_template"
%%% End: 
