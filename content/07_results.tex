%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%                                                                 %
%%%%%     <file_name>.tex                                             %
%%%%%                                                                 %
%%%%% Author:      <author>                                           %
%%%%% Created:     <date>                                             %
%%%%% Description: <description>                                      %
%%%%%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Results}
\section{Test Setup}
	I tested the Halide implementation using two applications, first the simple pipeline described in the listing~\ref{code:simple_pipeline} to check if everything was working and then with a matrix multiplication pipeline to be closer to a real scenario. Once I checked that Halide was working correctly.
	I ran a similar matrix multiplication program using \gls{openmp} to compare the parallelisation performance of Halide against the current toolchain available on \gls{hero}.
	To check the output of the Halide application, I compared the output of the pipeline to the precomputed result.
	During  all Halide benchmarks, the two input matrices were randomly generated using a python script, as the multiplication on the \gls{pulp} cluster always take two cycles, the content of the matrices does not impact the execution time of the programs.
	To get comparable results for both applications, stored the matrices in the L1 cache of \gls{hero} to reduce the memory latency.
	To measure the performance of both applications (on Halide and on \gls{openmp}), I used two functions of the \gls{hero} runtime: \texttt{hero\_reset\_clk\_counter()} and \texttt{hero\_get\_clk\_counter()}.
	These two functions respectively reset a cycle counter and return the counter's value when called. As their execution is short, we can get cycle accurate result for our benchmarks.
	The benchmarks were executed on the hardware simulator, as I did not have time to make the heterogenous compilation work.

\section{Halide Results}

% Table of the values
\begin{figure}[H]
		\center
		\scalebox{.8}{\input{figures_raw/Barplot_Halide.pgf}}
    \caption{Halide results relative to Halide base schedule performance}
	\label{fig:barPlotHalide}
\end{figure}



\begin{figure}[H]
		\center
	\scalebox{.8}{\input{figures_raw/Roofline_Halide.pgf}}
    \caption{Roofline plot for Halide}
	\label{fig:rooflineHalide}
\end{figure}

\input{content/BenchmarksHalide.tex}

	The figures~\ref{fig:barPlotHalide} and \ref{fig:rooflineHalide} show the results of Halide for different matrix sizes ranging from 15 to 40.
	The benchmarks were done using three different schedule: the default one, with no parallelisation, one schedule with parallelisation along the y axis, and one schedule which combines a parallel schedule with a vectorize one.
	On figure~\ref{fig:barPlotHalide}, we can see that the paralellisation instruction is efficient as this schedule performs between five and eight times better than the default schedule.

	Figure~\ref{fig:rooflineHalide} show the performance of Halide compared to an ideal scenario. As one matrix multiplication can be done in $2n^3$ arithmetic operations, in the best scenario, each core achieve one arithmetic operation per cycle, so we can get up to eight operations per cycle.

	For the default schedule, we can see that the overall performance stays consant and is not dependant on the matrix size. But for the two other schedule, the performance increase when we increase the matrix size. This can be explained by the core utilisation, as every core only executes the task when: \texttt{core\_id == task\_id \% n\_cores}, the overall core utilization is lower for smaller matrices when the matrix size is not a multiple of the number of cores. For a matrix of size twenty, four cores will execute three tasks, and four cores will execute two tasks.
	But for bigger matrices, core will stay inactive for a smaller fraction of the total execution time, increasing the overall performance of the  system.
 

\section{Comparison with an already working toolchain: \acrshort{openmp}}

\begin{figure}[H]
		\center
	\scalebox{.6}{\input{figures_raw/RooflineHalideOpenMp.pgf}}
    \caption{Roofline plot for Halide}
	\label{fig:rooflineHalideOpenMP}
\end{figure}
\begin{figure}[H]
		\center
		\scalebox{.6}{\input{figures_raw/BarplotHalideOpenMp.pgf}}
    \caption{Halide results relative to Halide base schedule performance}
	\label{fig:barPlotHalideOpenMP}
\end{figure}

\input{content/BenchmarksHalideOpenMP.tex}

As \gls{hero} already have a working workflow to distribute computation on the \gls{pulp} cluster using \gls{openmp}, we can see how Halide compare to this \gls{api} on  a parallel schedule.
	First, we can see that both implementation of the matrix multiplication achieve the same parformance, differing only by less than four thousand cycles for fourty by fourty matrices.

	The most interesting results come from the parallel code distribution by the two \glspl{api}.





%%%% OLD TEXT


	I made sure that the two matrices were stored in the L1 cache, to have the best access time possible.
	To measure the number of cycles needed to run the application, I used two functions available in the hero sdk: \texttt{hero\_reset\_clk\_counter()} and \texttt{hero\_get\_clk\_counter()}. These functions reset and return the value of a cycle counter. As they only take few assembly instructions, they are useful to get cycles accurate measurements of the execution time.
	With this setup, we can easily compare the performances of Halide and \gls{openmp} in a real world scenario for at least two basic schedules: single threaded and multi threaded. I then experimented with different schedule with Halide to see the maximal performance I could get with this application.

% CORRECT THE NUMBER OF Operations
	To give the results more meaning, I converted the benchmark data in operations per cycles where one operation can either be an addition or a multiplication, so for a matrix of size n, the number of operations to finish the multiplication is : $2n^3$.

\section{Comparaison between \acrshort{openmp} and Halide on the different platforms}



	Halide base schedule performs similarly from \gls{openmp} single threaded, differing only by a few thousands cycles for the last test (with two matrices of size 35 by 35).
	The two implementation are prettty similar, and Halide overhead is in this case almost negligeable.

	The second schedule I tested on both \glspl{api} was the parallel schedule; as parallelisation is the most efficient way to increase performance especially when there is no data dependancy in the pipeline. To have the best possible performance, the \gls{api} needs to be as small as possible.

	The table~\ref{Table:Benchmarks} show the results of both applications, with every size tested, we can see that Halide is in every situation at least $.2$ operations per cycle faster than \gls{openmp}.

%	To represent  the code efficiency, I used the number of useful operations per cycles. As we have eight core at our disposal, capable at most of one operation per cycle, we can achieve at best eight operations per cycle. The graph~\ref{fig:roofline}, represent the performance of the schedule relative to the computing intensity of the program (The number of operations required to output one byte of data). The computational intensity for a matrix of size n is $C_n = \frac{2n^3}{12n^2} = \frac{n}{6}$ where $2n^3$ is the number of operations, and $12n^2$ is the number of byte needed to complete the operation on 32 bits integers.

%	On the  graph~\ref{fig:roofline}, we can see that Halide with it's parallel schedule achieves better results than OpenMP on all the sizes tested( from fifteen by fifteen up to thirty five by thirty five). The gaps in performance stays between $.2$ operations per cycles and $.4$ operations per cycles depending on the size of the matrices.


I also tried multiple schedule for Halide to see the best performance \gls{hero} could achieve on this benchmark. I tried to combine the parallel schedule with loop unrolling or tiling, but I was only getting worse results due to the additional jumps or the additional computation implied by loop unrolling. When the unrolling factor is not a divisor of the number of loop iteration, Halide shift the final iteration to always compute the same number of element each iteration. This shift forces the pipeline to recompute some output values.

\lstset{basicstyle=\ttfamily\footnotesize,breaklines=true,tabsize=2}
\begin{lstlisting}[caption={Schedule using Parallel and Vectorize}, captionpos=b, label={code:good_schedule}]
	out.parallel(y);
	out.vectorize(x, 10);

\end{lstlisting}

	The vectorize schedule used with parallel proves to be the most efficient solution, on twenty by twenty matrices, the parallel schedule alone achieved $1.026$ operations per cycle, against $2.169$ operations per cycle using the schedule~\ref{code:good_schedule}.
	I then exhaustively tried every vectorization factor possible to see which performed best.

\begin{figure}[H]
	\begin{center}
	\resizebox{10cm}{!}{\input{figures_raw/Vectorize.pgf}}
	\label{Fig:Vectorization}
		\caption{Impact of the vectorization factor on the performance of the application}
	\end{center}
\end{figure}

%0_paraYVect_2/modelsimLog:   11619 Cycles
%1_paraYVect_3/modelsimLog:   10433 Cycles
%2_paraYVect_4/modelsimLog:   9240  Cycles
%3_paraYVect_5/modelsimLog:   8358  Cycles
%4_paraYVect_6/modelsimLog:   9333  Cycles
%5_paraYVect_7/modelsimLog:   8012  Cycles
%6_paraYVect_8/modelsimLog:   8442  Cycles
%7_paraYVect_9/modelsimLog:   9151  Cycles
%8_paraYVect_10/modelsimLog:  7376  Cycles
%9_paraYVect_11/modelsimLog:  7829  Cycles
%10_paraYVect_12/modelsimLog: 8049  Cycles
%11_paraYVect_13/modelsimLog: 10202 Cycles
%12_paraYVect_14/modelsimLog: 11387 Cycles
%13_paraYVect_15/modelsimLog: 12755 Cycles
%14_paraYVect_16/modelsimLog: 13894 Cycles
%15_paraYVect_17/modelsimLog: 15551 Cycles
%16_paraYVect_18/modelsimLog: 18007 Cycles
%17_paraYVect_19/modelsimLog: 18358 Cycles






%- Tried differents schedule to get better performances:
%        - parallel X, Y
%        - unrolling, tiling, and combination of both, but always worse performances
%- Best schedule to date (20 x 20 matrices):
%    - parallel over Y (15 595 cycles 1.0259 OPC)
%    - parallel over Y + vectorize X (10) (7376 cycles 2.169 OPC)
%
